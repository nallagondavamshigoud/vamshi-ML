{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_CNN.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ns0tj4r7GskZ","colab_type":"text"},"source":["**Convolution Neural Network(CNN)**:\n","Mainly convolution neural network is used to classify the imagedata set.\n","\n","GrayScale Image Classification:It Contaions 2-D ,input,According to Below Example Input Size is28x28,Now we need to Convolve the given Image with filters/Kernels,here convolve Operation was element wise multiplication  followed by addition,of square matrix preffered,not neccesarly,we can take some hardcore c\n","kernels,its better to learn by model by given input by using back propaagation techniqe, CNN filters are like weights in MLPs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kmqxof40NUcN","colab_type":"text"},"source":["**RGB Image Classification**:\n","                          The Image Dataset With RGB contains,3 Chaneels with values lies between 1-255,it is 3-D tensor,0f nxmxd,n=Height,m=width,d=Number of Channels,kernal must be same size of depth which means 3 channels for RGB image,when we conlove the kernel with input image,here we use slide operator,whichs after convolve with pixcel of intial points it slides to some value which given by us,thorugh vertically and horizantally,it gives an ouput 3-D tensor of pxsxl,were p<n,m<s,l= number of kernels ,if want to get same size of input shape,we need to do padding on input imagedata,which adding one row and column to image for either side and top and bottom of image,and fill these pixcels with zeros,we call it as zero padding,after getting output,we need to apply activation function on top of it,in our example we are going to use,RElU,Sigmoid functions,the output of firstlayer is input to next layer,after applying Two Convolution layer and activation Functions,we performs maxpolling,it reduces the shape, maxpolling also same like kernels,but it choose max value among the all of that particular,pixcels,here we need to give slide value,if not it tskes defult value 1,maxpolling creates image in-variance,which means if recive max value of particular pixcels it activates all those pixcels,the output of this layer is input to the next convolution layer,after performing reqired CNNs,we apply flatten operation ,which means it convert 3-D tensors to 1-D,by multipling all values of height,width and depth for EX:28x28x3= 2352,this value passes to dense layer ,on top of input to dense layer we apply softmax clasification,for multi classification,after clasification we have y_^,and it caliculates loss function with repect to the kernels based on preedictions,and upadates the kernels according to that,which minimizes the loss  "]},{"cell_type":"code","metadata":{"id":"LB8mrPCB8uF3","colab_type":"code","outputId":"c8d87e21-84ad-4cac-ac97-8a69acff7526","executionInfo":{"status":"ok","timestamp":1569730233763,"user_tz":-330,"elapsed":1879202,"user":{"displayName":"N. V. Goud","photoUrl":"","userId":"05680090607141295341"}},"colab":{"base_uri":"https://localhost:8080/","height":583}},"source":["\n","from __future__ import print_function\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 12\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/12\n","60000/60000 [==============================] - 161s 3ms/step - loss: 0.2603 - acc: 0.9193 - val_loss: 0.0580 - val_acc: 0.9803\n","Epoch 2/12\n","60000/60000 [==============================] - 157s 3ms/step - loss: 0.0854 - acc: 0.9741 - val_loss: 0.0409 - val_acc: 0.9864\n","Epoch 3/12\n","60000/60000 [==============================] - 163s 3ms/step - loss: 0.0657 - acc: 0.9811 - val_loss: 0.0351 - val_acc: 0.9875\n","Epoch 4/12\n","60000/60000 [==============================] - 157s 3ms/step - loss: 0.0531 - acc: 0.9838 - val_loss: 0.0307 - val_acc: 0.9893\n","Epoch 5/12\n","60000/60000 [==============================] - 157s 3ms/step - loss: 0.0460 - acc: 0.9857 - val_loss: 0.0305 - val_acc: 0.9896\n","Epoch 6/12\n","60000/60000 [==============================] - 154s 3ms/step - loss: 0.0401 - acc: 0.9875 - val_loss: 0.0270 - val_acc: 0.9908\n","Epoch 7/12\n","60000/60000 [==============================] - 154s 3ms/step - loss: 0.0374 - acc: 0.9884 - val_loss: 0.0274 - val_acc: 0.9908\n","Epoch 8/12\n","60000/60000 [==============================] - 155s 3ms/step - loss: 0.0336 - acc: 0.9899 - val_loss: 0.0278 - val_acc: 0.9914\n","Epoch 9/12\n","60000/60000 [==============================] - 152s 3ms/step - loss: 0.0308 - acc: 0.9907 - val_loss: 0.0257 - val_acc: 0.9922\n","Epoch 10/12\n","60000/60000 [==============================] - 154s 3ms/step - loss: 0.0284 - acc: 0.9916 - val_loss: 0.0279 - val_acc: 0.9912\n","Epoch 11/12\n","60000/60000 [==============================] - 153s 3ms/step - loss: 0.0282 - acc: 0.9914 - val_loss: 0.0277 - val_acc: 0.9904\n","Epoch 12/12\n","60000/60000 [==============================] - 154s 3ms/step - loss: 0.0274 - acc: 0.9919 - val_loss: 0.0255 - val_acc: 0.9911\n","Test loss: 0.025484359548266774\n","Test accuracy: 0.9911\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y7hYryA3VW_6","colab_type":"text"},"source":["Import Reqired packages,\n","batch_size,is importent ,becuse it decides number data points need to feed,num_classes is to classify by softmax laayer for target variable,in our mnist data their are 10 classes,so it is 10-class clasification,epochs =  number of itreations to perform,we can know shape of image by iput,in our example no,of rows are 28,and no.of columns are 28,it means 28x28 pixcels.\n","we sliptted the data into x train and xtest\n","In If else condition,if given data contains chanels first excute that code,it means reshape the xtrain data,in such a way that (shape of the x_tain,channel,no.of rows,no,of columns) else,\n","(shape of the x_tain,no.of rows,no,of columns,channel),in our it excuted else part,we assign xtrain and xtest data as flaot 32 datatype,\n","in training data values varies in between 1-255,so we need to normalize that ,by dividing the xtrain data with 255 because maximum value of data is 255 it comes to standadized formart.\n","next we converted categorical target class to binary class matrics\n","we created object to sequential(),\n","with that object we can add all required layersof CNN to sequential method,which executes sequentially one after one,i.e model.add(Conv2D(32, kernel_size=(3, 3),activation='relu'input_shape=input_shape)\n","in above line of code we added input_shape of image,and we given kernel_size,but not values in kernel,which its find by model by using back propagation,next number of kernels i.e 32 inabove line,and we given relu as a activation function,on top of output after convolve,after 2 CNNs we added maxpooling layer which reduce the shape,then we add dropout layer to avoid overfitting with some dropout rate,better choose below 0.5,here we choosen 0.25,0.25 activaction functions are swithed off,next layer flatten it means it convert 3-D tensor to 1-D tensor by multiplyingthe values,i.e heightXwidthXchannels,we sent these output densce layer which consist relu activation and we perform dropout to avoid overfitting,\n","and we pass this data to an ouput layer apply ,softmax classification on top of it,which classifies input datapoint any one of the num_classes,and caluculates the loss function and upadates the kernels ,which gives less loss,loss=(y-y^),before traing data we need to declare the loss function,optimizer and metric,by using compile method,after we need train the the by using fit method by passing xtrain,ytrain data,with some batchsize,we can pass validation data diffrentlly ,or in same fit model by passing values to validation_data,vales are xtest,ytest.\n","the next evaluate method to evaluate the scores on validation data,by passing xtest,ytest\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Txq7X4BqGau8","colab_type":"code","outputId":"e48d201f-efee-42ee-ceb2-7eb802860da5","executionInfo":{"status":"ok","timestamp":1569740063381,"user_tz":-330,"elapsed":4422,"user":{"displayName":"N. V. Goud","photoUrl":"","userId":"05680090607141295341"}},"colab":{"base_uri":"https://localhost:8080/","height":528}},"source":["from keras.layers.normalization import BatchNormalization\n","model = Sequential()\n","model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',input_shape=input_shape))\n","model.add(Conv2D(30, (4, 4), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(20, (2, 2), activation='relu'))\n","model.add(Conv2D(10, (2, 2), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(BatchNormalization())\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/12\n","60000/60000 [==============================] - 101s 2ms/step - loss: 0.2983 - acc: 0.9064 - val_loss: 0.0713 - val_acc: 0.9757\n","Epoch 2/12\n","60000/60000 [==============================] - 102s 2ms/step - loss: 0.0989 - acc: 0.9699 - val_loss: 0.0529 - val_acc: 0.9826\n","Epoch 3/12\n","60000/60000 [==============================] - 99s 2ms/step - loss: 0.0749 - acc: 0.9777 - val_loss: 0.0454 - val_acc: 0.9860\n","Epoch 4/12\n","60000/60000 [==============================] - 101s 2ms/step - loss: 0.0647 - acc: 0.9804 - val_loss: 0.0309 - val_acc: 0.9899\n","Epoch 5/12\n","60000/60000 [==============================] - 103s 2ms/step - loss: 0.0550 - acc: 0.9837 - val_loss: 0.0249 - val_acc: 0.9921\n","Epoch 6/12\n","60000/60000 [==============================] - 101s 2ms/step - loss: 0.0498 - acc: 0.9849 - val_loss: 0.0382 - val_acc: 0.9886\n","Epoch 7/12\n","60000/60000 [==============================] - 100s 2ms/step - loss: 0.0474 - acc: 0.9857 - val_loss: 0.0269 - val_acc: 0.9917\n","Epoch 8/12\n","60000/60000 [==============================] - 104s 2ms/step - loss: 0.0412 - acc: 0.9877 - val_loss: 0.0413 - val_acc: 0.9874\n","Epoch 9/12\n","60000/60000 [==============================] - 100s 2ms/step - loss: 0.0425 - acc: 0.9874 - val_loss: 0.0265 - val_acc: 0.9911\n","Epoch 10/12\n","60000/60000 [==============================] - 102s 2ms/step - loss: 0.0372 - acc: 0.9882 - val_loss: 0.0223 - val_acc: 0.9919\n","Epoch 11/12\n","60000/60000 [==============================] - 100s 2ms/step - loss: 0.0368 - acc: 0.9895 - val_loss: 0.0246 - val_acc: 0.9920\n","Epoch 12/12\n","60000/60000 [==============================] - 98s 2ms/step - loss: 0.0341 - acc: 0.9889 - val_loss: 0.0300 - val_acc: 0.9904\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sxTPRaLxWyOm","colab_type":"code","outputId":"27889225-6877-4dc5-cd0f-c4a09d6800d4","executionInfo":{"status":"ok","timestamp":1569741789354,"user_tz":-330,"elapsed":1473288,"user":{"displayName":"N. V. Goud","photoUrl":"","userId":"05680090607141295341"}},"colab":{"base_uri":"https://localhost:8080/","height":492}},"source":["model = Sequential()\n","model.add(Conv2D(20, kernel_size=(5, 5),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","model.add(Conv2D(30, (4, 4), activation='relu',strides=(1, 1),padding='same' ))\n","\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(20, (2, 2), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2) ))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/12\n","60000/60000 [==============================] - 126s 2ms/step - loss: 0.3347 - acc: 0.8947 - val_loss: 0.0616 - val_acc: 0.9794\n","Epoch 2/12\n","60000/60000 [==============================] - 119s 2ms/step - loss: 0.1124 - acc: 0.9679 - val_loss: 0.0552 - val_acc: 0.9811\n","Epoch 3/12\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0882 - acc: 0.9740 - val_loss: 0.0401 - val_acc: 0.9867\n","Epoch 4/12\n","60000/60000 [==============================] - 123s 2ms/step - loss: 0.0747 - acc: 0.9782 - val_loss: 0.0340 - val_acc: 0.9889\n","Epoch 5/12\n","60000/60000 [==============================] - 122s 2ms/step - loss: 0.0640 - acc: 0.9809 - val_loss: 0.0298 - val_acc: 0.9902\n","Epoch 6/12\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0591 - acc: 0.9827 - val_loss: 0.0249 - val_acc: 0.9918\n","Epoch 7/12\n","60000/60000 [==============================] - 123s 2ms/step - loss: 0.0555 - acc: 0.9838 - val_loss: 0.0248 - val_acc: 0.9922\n","Epoch 8/12\n","60000/60000 [==============================] - 124s 2ms/step - loss: 0.0515 - acc: 0.9850 - val_loss: 0.0233 - val_acc: 0.9924\n","Epoch 9/12\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0472 - acc: 0.9859 - val_loss: 0.0232 - val_acc: 0.9920\n","Epoch 10/12\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0436 - acc: 0.9873 - val_loss: 0.0233 - val_acc: 0.9923\n","Epoch 11/12\n","60000/60000 [==============================] - 123s 2ms/step - loss: 0.0429 - acc: 0.9870 - val_loss: 0.0239 - val_acc: 0.9920\n","Epoch 12/12\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0411 - acc: 0.9879 - val_loss: 0.0217 - val_acc: 0.9934\n"],"name":"stdout"}]}]}
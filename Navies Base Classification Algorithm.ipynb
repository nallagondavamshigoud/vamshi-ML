{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DonorsChoose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook vamshinallagonda@gmail_A4_NBT.ipynb to html\n",
      "[NbConvertApp] Writing 613089 bytes to vamshinallagonda@gmail_A4_NBT.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html vamshinallagonda@gmail_A4_NBT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right\n",
    "now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the\n",
    "DonorsChoose.org website.\n",
    "\n",
    "Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they\n",
    "need to solve:\n",
    "    \n",
    " How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as\n",
    " efficiently as possible\n",
    " How to increase the consistency of project vetting across different volunteers to improve the experience for teachers\n",
    " How to focus volunteer time on the applications that need the most assistance\n",
    "    \n",
    "The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be\n",
    "approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school.\n",
    "DonorsChoose.org can then use this information to identify projects most likely to need further review before approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamshi goud\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\vamshi goud\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 18)\n",
      "(4000, 18)\n",
      "(16000,)\n",
      "(4000,)\n",
      "(12800, 18)\n",
      "(3200, 18)\n",
      "(12800,)\n",
      "(3200,)\n"
     ]
    }
   ],
   "source": [
    "#project_data = pd.read_csv('train_data.csv',nrows=50000)\n",
    "data = pd.read_csv('train_data.csv',nrows=20000)\n",
    "data2 = pd.read_csv('resources.csv')\n",
    "data3 =data2.groupby('id').agg({'price':'sum','quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(data,data3,on='id',how='left')\n",
    "project_data['price'] = project_data['price'].fillna((project_data['price'].mean()))\n",
    "project_data['quantity'] = project_data['quantity'].fillna((project_data['quantity'].mean()))\n",
    "y = np.array(project_data['project_is_approved'])\n",
    "x = project_data.drop('project_is_approved',axis=1)\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y, test_size = 0.2, random_state =42)\n",
    "xTrain_c,x_CV,yTrain_c,y_CV   = train_test_split(xTrain,yTrain, test_size = 0.2, random_state =42)\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)\n",
    "print(yTrain.shape)\n",
    "print(yTest.shape)\n",
    "print(xTrain_c.shape)\n",
    "print(x_CV.shape)\n",
    "print(yTrain_c.shape)\n",
    "print(y_CV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160221</td>\n",
       "      <td>p253737</td>\n",
       "      <td>c90749f5d961ff158d4b4d1e7dc665fc</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140945</td>\n",
       "      <td>p258326</td>\n",
       "      <td>897464ce9ddc600bced1151f324dd63a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21895</td>\n",
       "      <td>p182444</td>\n",
       "      <td>3465aaf82da834c0582ebd0ef8040ca0</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-08-31 12:03:56</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness, Team Sports</td>\n",
       "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
       "      <td>\\r\\n\\\"True champions aren't always the ones th...</td>\n",
       "      <td>The students on the campus come to school know...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>p246581</td>\n",
       "      <td>f3cb9bffbba169bef1a77b243e620b60</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>KY</td>\n",
       "      <td>2016-10-06 21:16:17</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language, Math &amp; Science</td>\n",
       "      <td>Literacy, Mathematics</td>\n",
       "      <td>Techie Kindergarteners</td>\n",
       "      <td>I work at a unique school filled with both ESL...</td>\n",
       "      <td>My students live in high poverty conditions wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need to engage in Reading and Math...</td>\n",
       "      <td>4</td>\n",
       "      <td>232.90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172407</td>\n",
       "      <td>p104768</td>\n",
       "      <td>be1f7507a41f8479dc06f047086a39ec</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>TX</td>\n",
       "      <td>2016-07-11 01:10:09</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Interactive Math Tools</td>\n",
       "      <td>Our second grade classroom next year will be m...</td>\n",
       "      <td>For many students, math is a subject that does...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need hands on practice in mathemat...</td>\n",
       "      <td>1</td>\n",
       "      <td>67.98</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "0      160221  p253737  c90749f5d961ff158d4b4d1e7dc665fc           Mrs.   \n",
       "1      140945  p258326  897464ce9ddc600bced1151f324dd63a            Mr.   \n",
       "2       21895  p182444  3465aaf82da834c0582ebd0ef8040ca0            Ms.   \n",
       "3          45  p246581  f3cb9bffbba169bef1a77b243e620b60           Mrs.   \n",
       "4      172407  p104768  be1f7507a41f8479dc06f047086a39ec           Mrs.   \n",
       "\n",
       "  school_state project_submitted_datetime project_grade_category  \\\n",
       "0           IN        2016-12-05 13:43:57          Grades PreK-2   \n",
       "1           FL        2016-10-25 09:22:10             Grades 6-8   \n",
       "2           AZ        2016-08-31 12:03:56             Grades 6-8   \n",
       "3           KY        2016-10-06 21:16:17          Grades PreK-2   \n",
       "4           TX        2016-07-11 01:10:09          Grades PreK-2   \n",
       "\n",
       "            project_subject_categories     project_subject_subcategories  \\\n",
       "0                  Literacy & Language                     ESL, Literacy   \n",
       "1    History & Civics, Health & Sports  Civics & Government, Team Sports   \n",
       "2                      Health & Sports    Health & Wellness, Team Sports   \n",
       "3  Literacy & Language, Math & Science             Literacy, Mathematics   \n",
       "4                       Math & Science                       Mathematics   \n",
       "\n",
       "                                       project_title  \\\n",
       "0   Educational Support for English Learners at Home   \n",
       "1              Wanted: Projector for Hungry Learners   \n",
       "2  Soccer Equipment for AWESOME Middle School Stu...   \n",
       "3                             Techie Kindergarteners   \n",
       "4                             Interactive Math Tools   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "2  \\r\\n\\\"True champions aren't always the ones th...   \n",
       "3  I work at a unique school filled with both ESL...   \n",
       "4  Our second grade classroom next year will be m...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...             NaN   \n",
       "1  The projector we need for our school is very c...             NaN   \n",
       "2  The students on the campus come to school know...             NaN   \n",
       "3  My students live in high poverty conditions wi...             NaN   \n",
       "4  For many students, math is a subject that does...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need opportunities to practice beg...   \n",
       "1             NaN  My students need a projector to help with view...   \n",
       "2             NaN  My students need shine guards, athletic socks,...   \n",
       "3             NaN  My students need to engage in Reading and Math...   \n",
       "4             NaN  My students need hands on practice in mathemat...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects   price  quantity  \n",
       "0                                             0  154.60        23  \n",
       "1                                             7  299.00         1  \n",
       "2                                             1  516.85        22  \n",
       "3                                             4  232.90         4  \n",
       "4                                             1   67.98         4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer,OneHotEncoder\n",
    "df = x\n",
    "imputer = Imputer(missing_values='NaN',strategy='mean',axis=0)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing of training 'essay' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing of training 'essay' data\n",
    "\n",
    "xTrain[\"essay\"] = xTrain[\"project_essay_1\"].map(str) +\\\n",
    "                  xTrain[\"project_essay_2\"].map(str) + \\\n",
    "                  xTrain[\"project_essay_3\"].map(str) + \\\n",
    "                  xTrain[\"project_essay_4\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTrain['essay'].values[15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', \"doesn't\", 'here', 'did', 'into', 'have', 'wouldn', 'he', 'than', 'any', 'won', 'didn', 'haven', 'before', 'more', 'his', 'had', 'in', 'nor', \"aren't\", 'and', 'of', 'so', 'that', \"you'd\", \"don't\", 'all', 'each', 'not', 'ain', 'ourselves', 'the', 'then', 'until', \"weren't\", \"you're\", 'its', 'what', 'hasn', 'doesn', 'him', \"couldn't\", 'your', 's', \"mightn't\", 'itself', 'myself', \"shouldn't\", 'hadn', \"should've\", 'you', 'her', 'am', \"mustn't\", 'through', 'other', 'doing', 'were', 'was', 'be', 'off', 'above', 'are', 't', 'it', 'further', 'where', 'out', 'isn', 'yours', \"shan't\", \"you've\", 'my', 'they', 'themselves', \"that'll\", 'aren', 'having', 'those', \"hadn't\", 'an', 'theirs', 'there', 'most', \"haven't\", 'will', 'me', 'who', 'now', 'been', 'once', 'hers', 'such', 'or', 'if', 'couldn', 'o', 'because', 'herself', 'during', 're', 'mustn', 'on', 'at', 'm', 'yourselves', 'd', 'from', 'whom', 'do', 'himself', 'same', 'needn', 'being', 'for', 'while', 'how', \"isn't\", 'their', 'between', 'wasn', 'which', 'a', \"hasn't\", 'about', \"didn't\", \"wasn't\", 'we', \"won't\", 'does', 'them', 'yourself', 'down', 'ma', 'under', 'but', 'to', 'y', 'can', \"wouldn't\", 'when', 'is', 'these', 'mightn', 'against', 'don', 'our', 'll', 'weren', 'up', 'both', \"it's\", 'below', 'has', 'again', 'this', 'why', 'very', 'only', 've', 'by', 'few', 'shan', 'should', 'with', \"she's\", 'she', 'just', \"needn't\", 'ours', 'some', 'no', 'as', 'own', 'shouldn', 'too', \"you'll\", 'after', 'over'}\n"
     ]
    }
   ],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:06<00:00, 2488.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_essays_x = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTrain['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_essays_x.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_c[\"essay\"] = xTrain_c[\"project_essay_1\"].map(str) +\\\n",
    "                        xTrain_c[\"project_essay_2\"].map(str) + \\\n",
    "                       xTrain_c [\"project_essay_3\"].map(str) + \\\n",
    "                       xTrain_c[\"project_essay_4\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTrain_c['essay'].values[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12800/12800 [00:04<00:00, 2700.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_essays_xc = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTrain_c['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_essays_xc.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTrain['project_title'].values[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 24478.81it/s]\n"
     ]
    }
   ],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "from tqdm import tqdm\n",
    "preprocessed_title_xT = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTrain['project_title'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sen = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_title_xT.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTrain_c['project_title'].values[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 12800/12800 [00:00<00:00, 22748.36it/s]\n"
     ]
    }
   ],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "from tqdm import tqdm\n",
    "preprocessed_title_xC = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTrain_c['project_title'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sen = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_title_xC.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of test data\n",
    "xTest[\"essay\"] = xTest[\"project_essay_1\"].map(str) +\\\n",
    "                        xTest[\"project_essay_2\"].map(str) + \\\n",
    "                        xTest[\"project_essay_3\"].map(str) + \\\n",
    "                        xTest[\"project_essay_4\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTest['essay'].values[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:01<00:00, 2462.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_essays_xTest = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTest['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_essays_xTest.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preporcessing of cross validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_CV[\"essay\"] = x_CV[\"project_essay_1\"].map(str) +\\\n",
    "                        x_CV[\"project_essay_2\"].map(str) + \\\n",
    "                       x_CV[\"project_essay_3\"].map(str) + \\\n",
    "                        x_CV[\"project_essay_4\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(x_CV['essay'].values[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3200/3200 [00:01<00:00, 2293.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_essays_xcvT = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(x_CV['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_essays_xcvT.append(sent.lower().strip())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTest['project_title'].values[3500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTrain_c['project_title'].values[3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 21634.90it/s]\n"
     ]
    }
   ],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "from tqdm import tqdm\n",
    "preprocessed_title_xt = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTest['project_title'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent1 = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_title_xt.append(sent1.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 12800/12800 [00:00<00:00, 18454.35it/s]\n"
     ]
    }
   ],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "from tqdm import tqdm\n",
    "preprocessed_title_xt_c = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(xTrain_c['project_title'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent1 = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_title_xt_c.append(sent1.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(xTest['project_title'].values[3500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3200/3200 [00:00<00:00, 25614.85it/s]\n"
     ]
    }
   ],
   "source": [
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "from tqdm import tqdm\n",
    "preprocessed_title_cvt = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(x_CV['project_title'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent1)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent1_t = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_title_cvt.append(sent1.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sent = decontracted(x_CV['project_title'].values[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arts,': 34, 'Needs,': 57, 'Warmth,': 264, 'Care': 264, 'Hunger': 264, 'Civics,': 428, 'Sports,': 516, 'Civics': 629, 'History': 1057, 'Learning': 1070, 'Learning,': 1201, 'Science,': 1480, 'Arts': 1793, 'Music': 1827, 'The': 1827, 'Sports': 2104, 'Applied': 2271, 'Needs': 2447, 'Special': 2504, 'Health': 2620, 'Language,': 4005, 'Language': 5639, 'Science': 6054, 'Math': 7534, 'Literacy': 9644, '&': 22946}\n"
     ]
    }
   ],
   "source": [
    "categories = list(x['project_subject_categories'].values)\n",
    "# remove special characters from list of strings python:\n",
    "#https://stackoverflow.com/a/47301924/4084039\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_list = []\n",
    "for i in categories:\n",
    "    temp = \"\"\n",
    "# consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & H\n",
    "\n",
    "    if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Scienc\n",
    "        j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math &\n",
    "    temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "    temp = temp.replace('&','_') # we are replacing the & value into\n",
    "    cat_list.append(temp.strip())\n",
    "   # print(cat_list.)\n",
    "    #print(x.le)\n",
    "#xTrain.apply(lambda col: col.drop_duplicates().reset_index(drop=True))\n",
    "#xTrain('project_subject_categories') = cat_list\n",
    "   # x['clean_categories'] = cat_list\n",
    "    #project_data.drop(['project_subject_categories'], axis=1, inplace=True)\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in x['project_subject_categories'].values:\n",
    "    my_counter.update(word.split())\n",
    "cat_dict = dict(my_counter)\n",
    "sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n",
    "print(sorted_cat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_categories = list(project_data['project_subject_subcategories'].values)\n",
    "# remove special characters from list of strings python:\n",
    "#https://stackoverflow.com/a/47301924/4084039\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "sub_cat_list = []\n",
    "for i in sub_categories:\n",
    "   \n",
    "\n",
    "    temp = \"\"\n",
    "# consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & H\n",
    "    if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Scienc\n",
    "        j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(\n",
    "    j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math &\n",
    "    temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "    temp = temp.replace('&','_')\n",
    "    sub_cat_list.append(temp.strip())\n",
    "#project_data['clean_subcategories'] = sub_cat_list\n",
    "#project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n",
    "# count of all the words in corpus python: https://stackoverflow.com/a/22898595/4084039\n",
    "my_counter = Counter()\n",
    "for word in project_data['project_subject_subcategories'].values:\n",
    "    my_counter.update(word.split())\n",
    "sub_cat_dict = dict(my_counter)\n",
    "sorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of essay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfid = TfidfVectorizer()\n",
    "xtrain_tfidf_essay = vectorizer_tfid.fit(preprocessed_essays_x)\n",
    "xtrain_tfidf_essay = vectorizer_tfid.transform(preprocessed_essays_x)\n",
    "xtrain_tfidf_essay_cv = vectorizer_tfid.transform(preprocessed_essays_xc)\n",
    "xtest_tfidf_esssay = vectorizer_tfid.transform(preprocessed_essays_xTest)\n",
    "xtest_tfidf_esssay_cv = vectorizer_tfid.transform(preprocessed_essays_xcvT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorization of title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfid = TfidfVectorizer()\n",
    "xtrain_tfidf_title = vectorizer_tfid.fit(preprocessed_title_xT)\n",
    "xtrain_tfidf_title =vectorizer_tfid.transform(preprocessed_title_xT)\n",
    "xtrain_tfidf_title_cv = vectorizer_tfid.transform(preprocessed_title_xC)\n",
    "xtest_tfidf_title =vectorizer_tfid.transform(preprocessed_title_xt)\n",
    "xtest_tfidf_title_cv = vectorizer_tfid.transform(preprocessed_title_cvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorization of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(min_df=10,max_features=5000)\n",
    "vectorizer.fit(xTrain['project_subject_categories'].values)\n",
    "categories_one_hot_train = vectorizer.transform(xTrain['project_subject_categories'].values)\n",
    "categories_one_hot_test = vectorizer.transform(xTest['project_subject_categories'].values)\n",
    "categories_one_hot_cv = vectorizer.transform(xTrain_c['project_subject_categories'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10,max_features=5000)\n",
    "vectorizer.fit(xTrain['project_subject_subcategories'].values)\n",
    "sub_categories_one_hot_train = vectorizer.transform(xTrain['project_subject_subcategories'].values)\n",
    "sub_categories_one_hot_test = vectorizer.transform(xTest['project_subject_subcategories'].values)\n",
    "sub_categories_one_hot_cv = vectorizer.transform(xTrain_c['project_subject_subcategories'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counter = Counter()\n",
    "for state in project_data['school_state'].values:\n",
    "    my_counter.update(state.split())\n",
    "school_state_dict = dict(my_counter)\n",
    "sorted_school_state_dict = dict(sorted(school_state_dict.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10,max_features=5000)\n",
    "vectorizer.fit(xTrain['school_state'].values)\n",
    "school_state_categories_one_hot_train = vectorizer.transform(xTrain['school_state'].values)\n",
    "school_state_categories_one_hot_test = vectorizer.transform(xTest['school_state'].values)\n",
    "school_state_categories_one_hot_cv = vectorizer.transform(xTrain_c['school_state'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counter = Counter()\n",
    "for project_grade in project_data['project_grade_category'].values:\n",
    "    my_counter.update(project_grade.split())\n",
    "project_grade_dict = dict(my_counter)\n",
    "sorted_project_grade_dict = dict(sorted(project_grade_dict.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10,max_features=5000)\n",
    "vectorizer.fit(xTrain['project_grade_category'].values)\n",
    "project_grade_categories_one_hot_train = vectorizer.transform(xTrain['project_grade_category'].values)\n",
    "project_grade_categories_one_hot_test =vectorizer.transform(xTest['project_grade_category'].values)\n",
    "project_grade_categories_one_hot_cv = vectorizer.transform(xTrain_c['project_grade_category'].values)\n",
    "#print(project_grade_categories_one_hot_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counter = Counter()\n",
    "for teacher_prefix in project_data['teacher_prefix'].values:\n",
    "    teacher_prefix = str(teacher_prefix)\n",
    "    my_counter.update(teacher_prefix.split())\n",
    "teacher_prefix_dict = dict(my_counter)\n",
    "sorted_teacher_prefix_dict = dict(sorted(teacher_prefix_dict.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'mrs', 'ms', 'teacher']\n"
     ]
    }
   ],
   "source": [
    "## https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nanis-an-invalid-document/39308809#39308809\n",
    "vectorizer = CountVectorizer(min_df = 10,max_features = 5000)\n",
    "vectorizer.fit(xTrain['teacher_prefix'].values.astype(\"U\"))\n",
    "teacher_prefix_categories_one_hot_train =vectorizer.transform(xTrain['teacher_prefix'].values.astype(\"U\"))\n",
    "teacher_prefix_categories_one_hot_test =vectorizer.transform(xTest['teacher_prefix'].values.astype(\"U\"))\n",
    "teacher_prefix_categories_one_hot_cv =vectorizer.transform(xTrain_c['teacher_prefix'].values.astype(\"U\"))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "[[0.00157365]\n",
      " [0.00550778]\n",
      " [0.00026228]\n",
      " ...\n",
      " [0.0010491 ]\n",
      " [0.        ]\n",
      " [0.00052455]]\n",
      "(16000, 1) (16000,)\n",
      "(12800, 1) (3200,)\n",
      "(4000, 1) (4000,)\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(xTrain['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "prev_projects_train = normalizer.transform(xTrain['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "#rev_projects_train1 = normalizer.transform(xTrain['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "prev_projects_cv = normalizer.transform(xTrain_c['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "prev_projects_test = normalizer.transform(xTest['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "print(\"After vectorizations\")\n",
    "prev_projects_train = prev_projects_train.reshape(-1,1)\n",
    "prev_projects_cv = prev_projects_cv.reshape(-1,1)\n",
    "prev_projects_test= prev_projects_test.reshape(-1,1)\n",
    "print(prev_projects_train)\n",
    "#print(prev_projects_train1)\n",
    "print(prev_projects_train.shape, yTrain.shape)\n",
    "print(prev_projects_cv.shape, y_CV.shape)\n",
    "print(prev_projects_test.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00406679 0.00127087 0.00686271 ... 0.01270871 0.01321706 0.00177922]]\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(xTrain['quantity'].values.reshape(1,-1))\n",
    "quant_projects_train = normalizer.transform(xTrain['quantity'].values.reshape(1,-1))\n",
    "quant_projects_cv = normalizer.transform(xTrain_c['quantity'].values.reshape(1,-1))\n",
    "quant_projects_test = normalizer.transform(xTest['quantity'].values.reshape(1,-1))\n",
    "print(quant_projects_train)\n",
    "quant_projects_train = quant_projects_train.reshape(-1,1)\n",
    "quant_projects_cv = quant_projects_cv.reshape(-1,1)\n",
    "quant_projects_test = quant_projects_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00268353 0.00243484 0.00451044 ... 0.00036411 0.00020681 0.01051077]]\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(xTrain['price'].values.reshape(1,-1))\n",
    "price_projects_train = normalizer.transform(xTrain['price'].values.reshape(1,-1))\n",
    "price_projects_cv = normalizer.transform(xTrain_c['price'].values.reshape(1,-1))\n",
    "price_projects_test = normalizer.transform(xTest['price'].values.reshape(1,-1))\n",
    "print(price_projects_train)\n",
    "price_projects_train = price_projects_train.reshape(-1,1)\n",
    "price_projects_cv = price_projects_cv.reshape(-1,1)\n",
    "price_projects_test = price_projects_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 5000)\n"
     ]
    }
   ],
   "source": [
    "#vectorizing the text data\n",
    "vectorizer = CountVectorizer(min_df=10,max_features=5000)\n",
    "xtrain_bow_essay = vectorizer.fit(preprocessed_essays_x)\n",
    "xtrain_bow_essay = vectorizer.transform(preprocessed_essays_x)\n",
    "#xTrain_bow_essay_c = vectorizer.fit(preprocessed_essays_xc)\n",
    "xtrain_bow_essay_c = vectorizer.transform(preprocessed_essays_xc)\n",
    "xtest_bow_esssay = vectorizer.transform(preprocessed_essays_xTest)\n",
    "xtest_bow_esssay_cv = vectorizer.transform(preprocessed_essays_xcvT)\n",
    "print(xtest_bow_esssay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorzing the title text data\n",
    "vectorizer = CountVectorizer(min_df=10,max_features=5000)\n",
    "xtrain_bow_title = vectorizer.fit(preprocessed_title_xT)\n",
    "xtrain_bow_title =vectorizer.transform(preprocessed_title_xT)\n",
    "#xTrain_bow_title_c = vectorizer.fit(preprocessed_title_xt_c)\n",
    "xtrain_bow_title_c = vectorizer.transform(preprocessed_title_xt_c)\n",
    "xtest_bow_title =vectorizer.transform(preprocessed_title_xt)\n",
    "xtest_bow_title_cv = vectorizer.transform(preprocessed_title_cvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking all features  of  training data with bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 6010)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "features=hstack((prev_projects_train,project_grade_categories_one_hot_train,categories_one_hot_train,teacher_prefix_categories_one_hot_train,sub_categories_one_hot_train,xtrain_bow_essay,xtrain_bow_title,quant_projects_train,price_projects_train)).tocsr()\n",
    "features = features.toarray()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking all features of cross validate data with bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 6010)\n"
     ]
    }
   ],
   "source": [
    "features_bow = hstack((quant_projects_cv,price_projects_cv,prev_projects_cv,project_grade_categories_one_hot_cv,categories_one_hot_cv,teacher_prefix_categories_one_hot_cv,sub_categories_one_hot_cv,xtrain_bow_essay_c,xtrain_bow_title_c))\n",
    "features_bow = features_bow.toarray()\n",
    "print(features_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking all features  of  test data with bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 6010)\n"
     ]
    }
   ],
   "source": [
    "xTest_features = hstack((quant_projects_test,price_projects_test,prev_projects_test,project_grade_categories_one_hot_test,categories_one_hot_test,teacher_prefix_categories_one_hot_test,sub_categories_one_hot_test,xtest_bow_title,xtest_bow_esssay)).tocsr()\n",
    "xTest_features = xTest_features.toarray()\n",
    "print(xTest_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "train_auc = []\n",
    "cv_auc = []\n",
    "kl = np.log([10**x for x in range (-4,4)])\n",
    "\n",
    "\n",
    "neigh = MultinomialNB(class_prior=[0.5,0.5])\n",
    "\n",
    "#clf = MultinomialNB(alpha=[10*2,2)], class_prior=None, fit_prior=True)\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "parameters = {'alpha':k}   \n",
    "clf = GridSearchCV(neigh,param_grid = parameters, cv=3, scoring='roc_auc')\n",
    "clf.fit(features, yTrain)\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "y_train_pred = loaded_model .predict_proba(features)[:,1]\n",
    "y_cv_pred =  loaded_model.predict_proba(features_bow)[:,1]\n",
    "    \n",
    "train_auc.append(roc_auc_score(yTrain,y_train_pred))\n",
    "cv_auc.append(roc_auc_score(yTrain_c, y_cv_pred))\n",
    "train_auc= loaded_model.cv_results_['mean_train_score']\n",
    "train_auc_std= loaded_model.cv_results_['std_train_score']\n",
    "cv_auc = loaded_model.cv_results_['mean_test_score'] \n",
    "cv_auc_std= loaded_model.cv_results_['std_test_score']\n",
    "\n",
    "kl = np.log([10**x for x in range (-4,4)])\n",
    "\n",
    "plt.plot(kl, train_auc, label='Train AUC')\n",
    "#this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n",
    "\n",
    "plt.gca().fill_between(kl,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n",
    "plt.plot(kl, cv_auc, label='CV AUC')\n",
    "plt.gca().fill_between(kl,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n",
    "plt.legend()\n",
    "plt.xlabel(\"K: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/47582264/python-how-to-convert-a-list-to-loglist\n",
    "kl = np.log([10**x for x in range (-4,4)])\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model .best_score_)\n",
    "print(loaded_model .best_params_)\n",
    "print(loaded_model .best_estimator_)\n",
    "y_test_pred = loaded_model .predict_proba(xTest_features)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) By Observing cross validate data point in above plot,the maximunm best score is 0.68            \n",
    "2) For that maximum score the best hyperparameter i,e alpha is 0.1 for multinominal NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "neigh = MultinomialNB(alpha=0.1, class_prior=[0.5,0.5], fit_prior=True)\n",
    "final1 = neigh.fit(features, yTrain)\n",
    "final = neigh.predict_proba(xTest_features)\n",
    "print(final)\n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(yTrain, y_train_pred)\n",
    "test_tpr, test_fpr, te_thresholds = roc_curve(yTest, y_test_pred)\n",
    "plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"True Positive Rate(TPR)\")\n",
    "plt.ylabel(\"False Positive Rate(FPR)\")\n",
    "plt.title(\"AUC\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) By  observing above plot area under curve (AUC),is above 0.60,it means model performing better,beacuse its above average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# we are writing our own function for predict, with defined thresould\n",
    "# we will pick a threshold that will give the least fpr\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "print(\"=\"*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(yTrain, predict_with_best_t(y_train_pred, best_t)))\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(yTest, predict_with_best_t(y_test_pred, best_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) From above confution matrix,we can say that ,the model predicted 2540 as true positive and 14429 as true negitive in train data\n",
    "\n",
    "2) In test data it predicted 952 as true postive and 110 as true neagative,as per plot it predicts 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heatmap_matrix =pd.DataFrame(confusion_matrix(yTrain, predict_with_best_t(y_train_pred, best_t)) , columns=[\"actual values:NO\",'actual value:YES'])\n",
    "print(heatmap_matrix)\n",
    "sns.heatmap(heatmap_matrix,annot=True, fmt='g',xticklabels='auto',yticklabels='auto')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heatmap_matrix_t =pd.DataFrame(confusion_matrix(yTest, predict_with_best_t(y_test_pred, best_t)),columns=[\"actual values:NO\",'actual value:YES'])\n",
    "print(heatmap_matrix_t)\n",
    "sns.set()\n",
    "sns.heatmap(heatmap_matrix_t,annot=True, fmt='g')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "features_tfid=hstack((price_projects_train,quant_projects_train,prev_projects_train,project_grade_categories_one_hot_train,categories_one_hot_train,teacher_prefix_categories_one_hot_train,sub_categories_one_hot_train,xtrain_tfidf_essay,xtrain_tfidf_title)).tocsr()\n",
    "features_tfid = features_tfid.toarray()\n",
    "print(features_tfid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cv_tfid = hstack((price_projects_cv,quant_projects_cv,prev_projects_cv,project_grade_categories_one_hot_cv,categories_one_hot_cv,teacher_prefix_categories_one_hot_cv,sub_categories_one_hot_cv,xtrain_tfidf_essay_cv,xtrain_tfidf_title_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cv_tfid = features_cv_tfid.toarray()\n",
    "print(features_cv_tfid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest_features_tfid = hstack((price_projects_test,quant_projects_test,prev_projects_test,project_grade_categories_one_hot_test,categories_one_hot_test,teacher_prefix_categories_one_hot_test,sub_categories_one_hot_test,xtest_tfidf_esssay,xtest_tfidf_title)).tocsr()\n",
    "xTest_features_tfid = xTest_features_tfid.toarray()\n",
    "print(xTest_features_tfid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#k=[10**x for x in range (-4,4)]\n",
    "kl = np.log([10**x for x in range (-4,4)])\n",
    "#alphas=10*(-2,2)\n",
    "\n",
    "neigh = MultinomialNB()\n",
    "#clf.fit(features, yTrain)\n",
    "#clf = MultinomialNB(alpha=[10*(-2,2)], class_prior=None, fit_prior=True)\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "parameters = {'alpha':k1}   \n",
    "clf1 = GridSearchCV(neigh,param_grid = parameters, cv=3, scoring='roc_auc')\n",
    "clf1.fit(features_tfid, yTrain)\n",
    "filename = 'finalized_model.sav'\n",
    "#model, open(filename, 'wb')\n",
    "pickle.dump(clf1, open(filename,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc1 = []\n",
    "cv_auc1 = []\n",
    "loaded_model1 = pickle.load(open(filename, 'rb'))\n",
    "y_train_pred_tfid =  loaded_model1.predict_proba(features_tfid)[:,1]\n",
    "y_cv_pred_tfid =  loaded_model1.predict_proba(features_cv_tfid)[:,1]\n",
    "    \n",
    "train_auc1.append(roc_auc_score(yTrain,y_train_pred_tfid))\n",
    "cv_auc1.append(roc_auc_score(yTrain_c, y_cv_pred_tfid))\n",
    "train_auc= loaded_model1.cv_results_['mean_train_score']\n",
    "train_auc_std= loaded_model1.cv_results_['std_train_score']\n",
    "cv_auc = loaded_model1.cv_results_['mean_test_score'] \n",
    "cv_auc_std= loaded_model1.cv_results_['std_test_score']\n",
    "\n",
    "plt.plot(kl, train_auc, label='Train AUC')\n",
    "plt.gca().fill_between(k1,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n",
    "plt.plot(kl, cv_auc, label='CV AUC')\n",
    "plt.gca().fill_between(k1,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n",
    "plt.legend()\n",
    "plt.xlabel(\"K: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model1.best_score_)\n",
    "print(loaded_model1.best_params_)\n",
    "print(loaded_model1.best_estimator_)\n",
    "y_test_pred_tfid = loaded_model1.predict_proba(xTest_features_tfid)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "neigh = MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
    "final1 = neigh.fit(features_tfid, yTrain)\n",
    "final = neigh.predict_proba(xTest_features_tfid)\n",
    "print(final)\n",
    "train_fpr1, train_tpr1, tr_thresholds1 = roc_curve(yTrain, y_train_pred_tfid)\n",
    "test_fpr1, test_tpr1, te_thresholds1 = roc_curve(yTest, y_test_pred_tfid)\n",
    "plt.plot(train_fpr1, train_tpr1, label=\"Train AUC =\"+str(auc(train_fpr1, train_tpr1)))\n",
    "plt.plot(test_fpr1, test_tpr1, label=\"Test AUC =\"+str(auc(test_fpr1, test_tpr1)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"True Positive Rate(TPR)\")\n",
    "plt.ylabel(\"False Positive Rate(FPR)\")\n",
    "plt.title(\"AUC\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# we are writing our own function for predict, with defined thresould\n",
    "# we will pick a threshold that will give the least fpr\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "print(\"=\"*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#best_t = find_best_threshold(tr_thresholds, train_fpr1, train_tpr1)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(yTrain, predict_with_best_t(y_train_pred_tfid, best_t)))\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(yTest, predict_with_best_t(y_test_pred_tfid, best_t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heatmap_matrix =pd.DataFrame(confusion_matrix(yTrain, predict_with_best_t(y_train_pred_tfid, best_t)),columns=[\"actual values:NO\",'actual value:YES'])\n",
    "print(heatmap_matrix)\n",
    "sns.heatmap(heatmap_matrix,annot=True, fmt='g')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heatmap_matrix =pd.DataFrame(confusion_matrix(yTrain, predict_with_best_t(y_train_pred_tfid, best_t)),columns=[\"actual values:NO\",'actual value:YES'])\n",
    "print(heatmap_matrix)\n",
    "sns.heatmap(heatmap_matrix,annot=True, fmt='g')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "#If you get a ModuleNotFoundError error , install prettytable using: pip3 install prettytable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Vectorizer\", \"Model\", \"Hyper Parameter\", \"AUC\"]\n",
    "x.add_row([\"BOW\", \"Brute\", 1, 0.49])\n",
    "x.add_row([\"TFIDF\", \"Brute\", 1, 0.60])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## just used for randomsearchcv again\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "train_auc = []\n",
    "cv_auc = []\n",
    "k = [10**x for x in range (-4,4)]\n",
    "#alphas=10*(-2,2)\n",
    "\n",
    "neigh = MultinomialNB(class_prior=[0.5,0.5])\n",
    "#clf.fit(features, yTrain)\n",
    "#clf = MultinomialNB(alpha=[10*(-2,2)], class_prior=None, fit_prior=True)\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "parameters = {\n",
    "    'alpha':[10**x for x in range (-4,4)]\n",
    "}   \n",
    "clf_R = RandomizedSearchCV(neigh,param_distributions = parameters, n_iter=8, cv=5, scoring='roc_auc')\n",
    "clf_R.fit(features_tfid, yTrain)\n",
    "\n",
    "y_train_pred_tfid =  clf_R.predict_proba(features_tfid)[:,1]\n",
    "y_cv_pred_tfid =  clf_R.predict_proba(features_cv_tfid)[:,1]\n",
    "    \n",
    "train_auc.append(roc_auc_score(yTrain,y_train_pred_tfid))\n",
    "cv_auc.append(roc_auc_score(yTrain_c, y_cv_pred_tfid))\n",
    "train_auc= clf_R.cv_results_['mean_train_score']\n",
    "train_auc_std= clf_R.cv_results_['std_train_score']\n",
    "cv_auc = clf_R.cv_results_['mean_test_score'] \n",
    "cv_auc_std= clf_R.cv_results_['std_test_score']\n",
    "\n",
    "plt.plot(kl, train_auc, label='Train AUC')\n",
    "plt.gca().fill_between(k,train_auc - train_auc_std,train_auc + train_auc_std,log(alpha=0.2),color='darkblue')\n",
    "plt.plot(kl, cv_auc, label='CV AUC')\n",
    "plt.gca().fill_between(k,cv_auc - cv_auc_std,cv_auc + cv_auc_std,log(alpha=0.2),color='darkorange')\n",
    "plt.legend()\n",
    "plt.xlabel(\"K: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(clf_R.best_score_)\n",
    "print(clf_R.best_params_)\n",
    "print(clf_R.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
